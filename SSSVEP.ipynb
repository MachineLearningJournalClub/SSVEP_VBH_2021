{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SSSVEP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_IW7lnnw04z"
      },
      "source": [
        "!pip install tslearn\n",
        "!pip install ydata-synthetic\n",
        "!pip install tsaug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S3nBkexOIPU"
      },
      "source": [
        "from keras import *\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tsaug.visualization import plot\n",
        "from tsaug import TimeWarp, Crop, Quantize, Drift, Reverse\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, TimeDistributed, AveragePooling1D, LSTM, Conv1D, Flatten, SeparableConv2D, Dropout, DepthwiseConv2D, MaxPooling2D, Activation, BatchNormalization, MaxPooling1D, Softmax, Input, Conv2D, AveragePooling2D\n",
        "from numpy import genfromtxt\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "import importlib\n",
        "import sys  \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sklearn.cluster as cluster\n",
        "import matplotlib.pyplot as plt\n",
        "from tslearn.generators import random_walks\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from ydata_synthetic.synthesizers.timeseries import TimeGAN\n",
        "\n",
        "os.environ['PYTHONHASHSEED']='0'\n",
        "np.random.seed(1)\n",
        "rn.seed(2)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87PNzwGVqbj9"
      },
      "source": [
        "#### READ DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ4vckjO9owh"
      },
      "source": [
        "mat = scipy.io.loadmat('subj1_train1.mat')\n",
        "trials = np.swapaxes(mat['data'],0,2)\n",
        "trials = np.swapaxes(trials,1,2)\n",
        "\n",
        "mat1 = scipy.io.loadmat('subj1_train2.mat')\n",
        "\n",
        "trials1 = np.swapaxes(mat1['data'],0,2)\n",
        "trials1 = np.swapaxes(trials1,1,2)\n",
        "\n",
        "mat2 = scipy.io.loadmat('subj2_train1.mat')\n",
        "\n",
        "trials2 = np.swapaxes(mat2['data'],0,2)\n",
        "trials2 = np.swapaxes(trials2,1,2)\n",
        "\n",
        "mat3 = scipy.io.loadmat('subj2_train2.mat')\n",
        "\n",
        "trials3 = np.swapaxes(mat3['data'],0,2)\n",
        "trials3 = np.swapaxes(trials3,1,2)\n",
        "\n",
        "trials = np.concatenate([trials, trials1, trials2, trials3])\n",
        "\n",
        "labels_map = {0: '9Hz', 1: '10Hz', \n",
        "                2: '12Hz', 3: '15Hz'}\n",
        "\n",
        "alt_labels_map = {0: 'TOP', 1: 'RIGHT', \n",
        "                2: 'BOTTOM', 3: 'LEFT'}\n",
        "\n",
        "def read_labels (file_path, labels_map):\n",
        "    oneHot_labels=genfromtxt(file_path,delimiter=' ')\n",
        "    labels = np.argmax(oneHot_labels, axis=1)\n",
        "\n",
        "    return labels, [labels_map[idx] for idx in labels]\n",
        "\n",
        "labels = read_labels('classInfo_4_5.m', labels_map)\n",
        "\n",
        "lab = labels[0].tolist()\n",
        "for x in labels[0]:\n",
        "  lab.append(x)\n",
        "  lab.append(x)\n",
        "  lab.append(x)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrHPrwNdOp37"
      },
      "source": [
        "plot(trials[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0HT3UA3qeL0"
      },
      "source": [
        "#### TIMESERIES AUGMENTATION\n",
        "Compute 10 new trials from every trial.\n",
        "\n",
        "TODO:\n",
        "\n",
        "\n",
        "*   Test other timeseries augmenter \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHdOnM4kN6aG"
      },
      "source": [
        "my_augmenter = (\n",
        "TimeWarp() * 15  # random time warping 15 times in parallel\n",
        "+ Crop(size=1882)  # random crop subsequences with length 300\n",
        "+ Quantize(n_levels=[10, 20, 30])  # random quantize to 10-, 20-, or 30- level sets\n",
        "+ Drift(max_drift=(0.1, 0.5)) @ 0.8  # with 80% probability, random drift the signal up to 10% - 50%\n",
        "+ Reverse() @ 0.5  # with 50% probability, reverse the sequence\n",
        ")\n",
        "trials_synt = []\n",
        "labels_synt = []\n",
        "for x in range(len(trials)):\n",
        "  i = my_augmenter.augment(trials[x])\n",
        "  ls1 = []\n",
        "  for t in range(i.shape[0]):\n",
        "    ls1.append(i[t])\n",
        "    if len(ls1) == 8:\n",
        "      trials_synt.append(ls1)\n",
        "      ls1 = []\n",
        "      labels_synt.append(lab[x])\n",
        "\n",
        "trials_synt = np.array(trials_synt)\n",
        "trials_synt.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MfG9ccXhfxg"
      },
      "source": [
        "plot(trials_synt[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq8FQ90zrMI3"
      },
      "source": [
        "#### RESHAPE AND CREATE TRAIN/TEST DATA FOR CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u_7XCx4jCPh"
      },
      "source": [
        "x_trials_synt = np.resize(trials_synt, (1200, 8, 1882))\n",
        "y_train = to_categorical(labels_synt)\n",
        "\n",
        "\n",
        "trials1 = np.resize(trials, (80, 8, 1882))\n",
        "x_train_ft, x_test_ft, y_train_ft, y_test_ft = train_test_split(trials1, lab, test_size= 0.2, random_state=42)\n",
        "y_test_ft = to_categorical(y_test_ft)\n",
        "y_train_ft = to_categorical(y_train_ft)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pafqzQicrpIZ"
      },
      "source": [
        "#### DEFINE, TRAIN, FINE-TUNE, TEST OF CNN\n",
        "TODO:\n",
        "\n",
        "\n",
        "*   Hyperparameters search\n",
        "*   Different architectures\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLie1bIJLzNr"
      },
      "source": [
        "# DEFINE HYPERPARAMETERS FOR CLASSIFICATION\n",
        "nb_classes = 4\n",
        "Chans = 8\n",
        "Samples = 1882 \n",
        "dropoutRate = 0.5\n",
        "kernLength = 10\n",
        "F1 = 256 \n",
        "D = 1\n",
        "F2 = 256\n",
        "dropoutType = Dropout\n",
        "\n",
        "#DEFINE MODEL\n",
        "\n",
        "input1   = Input(shape = (8, 1882, 1))\n",
        "block1  = Conv2D(F1, (1, kernLength), padding = 'same',\n",
        "                  input_shape = (Chans, Samples, 1))(input1)\n",
        "block1 = BatchNormalization()(block1)\n",
        "block1 = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
        "                          depth_multiplier = D,\n",
        "                          depthwise_constraint = max_norm(1.))(block1)\n",
        "block1 = BatchNormalization()(block1)\n",
        "block1= Activation('elu')(block1)\n",
        "block1= AveragePooling2D((1, 4))(block1)\n",
        "block1= dropoutType(dropoutRate)(block1)\n",
        "    \n",
        "block2= SeparableConv2D(F2, (1, 16), padding = 'same')(block1)\n",
        "block2= BatchNormalization()(block2)\n",
        "block2= Activation('elu')(block2)\n",
        "block2= AveragePooling2D((1, 8))(block2)\n",
        "block2= dropoutType(dropoutRate)(block2)\n",
        "        \n",
        "flatten= Flatten(name = 'flatten')(block2)\n",
        "dense1 = Dense(600, name = 'dense1')(flatten)\n",
        "\n",
        "dense= Dense(nb_classes, name = 'dense')(dense1)\n",
        "softmax= Activation('softmax', name = 'softmax')(dense)\n",
        "    \n",
        "model = Model(inputs=input1, outputs=softmax)\n",
        "\n",
        "#COMPILE MODEL\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "#TRAIN ON SYNTHETIC DATA\n",
        "model.fit(trials_synt, y_train, epochs = 40, batch_size=16)\n",
        "model.evaluate(x_test_ft, y_test_ft)\n",
        "\n",
        "#FINE-TUNE ON TRIAL DATA\n",
        "model.fit(x_train_ft, y_train_ft, epochs = 10, batch_size=1)\n",
        "model.evaluate(x_test_ft, y_test_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioyE9rCxsJDd"
      },
      "source": [
        "#### SCALE DATA TO [-1, 1]\n",
        "\n",
        "TODO:\n",
        "\n",
        "\n",
        "*   Different scalers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiUIgeen3BMq"
      },
      "source": [
        "x_train = np.resize(x_trials_synt, [800, 1882, 8])\n",
        "ls = []\n",
        "for x in range(len(x_train1)):\n",
        "  scaler = MinMaxScaler()\n",
        "  train_data = scaler.fit_transform(x_train1[x])\n",
        "  ls.append(train_data)\n",
        "\n",
        "x_train_gan = np.array(ls)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N7AMCanscKK"
      },
      "source": [
        "#### DEFINE HYPERPARAMETERS FOR TIMEGAN\n",
        "TODO:\n",
        "\n",
        "*   Hyperparameters search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfGojlk4jilA"
      },
      "source": [
        "model = TimeGAN\n",
        "\n",
        "seq_len = 1882\n",
        "n_seq = 8\n",
        "noise_dim = 8\n",
        "dim = 16\n",
        "batch_size = 256\n",
        "log_step = 100\n",
        "learning_rate = 5e-4\n",
        "\n",
        "gan_args = [batch_size, learning_rate, noise_dim, 1882, 2, (0, 1), dim]\n",
        "models_dir = './cache'\n",
        "synthesizer = model(gan_args, hidden_dim=8, seq_len=seq_len, n_seq=n_seq, gamma=0.1)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ZfBmMrsr2z"
      },
      "source": [
        "#### TRAIN TIMEGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUItrOqSijlq"
      },
      "source": [
        "synthesizer.train(x_train_gan, train_steps=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUivWmIRsuvw"
      },
      "source": [
        "#### SYNTHETIZE NEW DATA FROM TIMEGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHKWgdIc_dn-"
      },
      "source": [
        "data_sample = synthesizer.sample(10000)\n",
        "data_sample[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIBasyWjjMvT"
      },
      "source": [
        "data_sample1 = []\n",
        "for x in data_sample:\n",
        "  data_sample1.append(scaler.inverse_transform(x))\n",
        "data_sample1[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgsA8V9PxRDI"
      },
      "source": [
        "data_sample1[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-BHOEVbyXoU"
      },
      "source": [
        "#### CREATE LABELS FOR SYNTHETIC DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzqztjHLwudX"
      },
      "source": [
        "km = TimeSeriesKMeans(n_clusters=4, metric=\"dtw\", max_iter=100,\n",
        "                       random_state=0).fit(trials)\n",
        "pred = km.predict(data_sample1)\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo8INpf6KK3t"
      },
      "source": [
        "test = np.resize(data_sample1[0], (8, 1882))\n",
        "plot(test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZaFYiYYzZvU"
      },
      "source": [
        "#### PREPARE DATA FOR CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZHaiunMzgRH"
      },
      "source": [
        "trials_synt = np.resize(data_sample1, (10004, 8, 1882))\n",
        "y_train_synt = to_categorical(pred)\n",
        "\n",
        "trials1 = np.resize(trials, (80, 8, 1882))\n",
        "x_train_ft, x_test_ft, y_train_ft, y_test_ft = train_test_split(trials1, lab, test_size= 0.2, random_state=42)\n",
        "y_test_ft = to_categorical(y_test_ft)\n",
        "y_train_ft = to_categorical(y_train_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwklGGOnzpf6"
      },
      "source": [
        "####DEFINE, TRAIN, FINE-TUNE, TEST CNN\n",
        "\n",
        "TODO:\n",
        "\n",
        "\n",
        "*   Search for hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB7QdgL6CEV7"
      },
      "source": [
        "nb_classes = 4\n",
        "Chans = 8\n",
        "Samples = 1882 \n",
        "dropoutRate = 0.5\n",
        "kernLength = 10\n",
        "F1 = 256 \n",
        "D = 1\n",
        "F2 = 256\n",
        "dropoutType = Dropout\n",
        "\n",
        "input1   = Input(shape = (8, 1882, 1))\n",
        "\n",
        "##################################################################\n",
        "block1 = Conv2D(F1, (1, kernLength), input_shape = (Chans, Samples, 1))(input1)\n",
        "block1 = BatchNormalization()(block1)\n",
        "block1 = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
        "                          depth_multiplier = D,\n",
        "                          depthwise_constraint = max_norm(1.))(block1)\n",
        "block1 = BatchNormalization()(block1)\n",
        "block1= Activation('elu')(block1)\n",
        "block1= AveragePooling2D((1, 4))(block1)\n",
        "block1= dropoutType(dropoutRate)(block1)\n",
        "    \n",
        "block2= SeparableConv2D(F2, (1, 16), padding = 'same')(block1)\n",
        "block2= BatchNormalization()(block2)\n",
        "block2= Activation('elu')(block2)\n",
        "block2= AveragePooling2D((1, 8))(block2)\n",
        "block2= dropoutType(dropoutRate)(block2)\n",
        "        \n",
        "flatten= Flatten(name = 'flatten')(block2)\n",
        "dense1 = Dense(1000, name = 'dense1')(flatten)\n",
        "\n",
        "dense= Dense(nb_classes, name = 'dense')(dense1)\n",
        "softmax= Activation('softmax', name = 'softmax')(dense)\n",
        "    \n",
        "model = Model(inputs=input1, outputs=softmax)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "model.fit(trials_synt, y_train, epochs = 1, batch_size=128)\n",
        "model.evaluate(x_test_ft, y_test_ft)\n",
        "model.fit(x_train_ft, y_train_ft, epochs = 10, batch_size=1)\n",
        "model.evaluate(x_test_ft, y_test_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu2x52xGElN4"
      },
      "source": [
        "\n",
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn, \n",
        "        distillation_loss_fn,\n",
        "        alpha,\n",
        "        temperature,\n",
        "    ):\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "   \n",
        "        with tf.GradientTape() as tape:\n",
        "            student_predictions = self.student(x, training=True)\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "             )\n",
        "\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        y_prediction = self.student(x, training=False)\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}